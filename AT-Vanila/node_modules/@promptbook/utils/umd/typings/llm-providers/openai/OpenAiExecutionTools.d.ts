import type { AvailableModel } from '../../execution/LlmExecutionTools';
import type { LlmExecutionTools } from '../../execution/LlmExecutionTools';
import type { PromptChatResult } from '../../execution/PromptResult';
import type { PromptCompletionResult } from '../../execution/PromptResult';
import type { Prompt } from '../../types/Prompt';
import type { OpenAiExecutionToolsOptions } from './OpenAiExecutionToolsOptions';
/**
 * Execution Tools for calling OpenAI API.
 */
export declare class OpenAiExecutionTools implements LlmExecutionTools {
    private readonly options;
    /**
     * OpenAI API client.
     */
    private readonly client;
    /**
     * Creates OpenAI Execution Tools.
     *
     * @param options which are relevant are directly passed to the OpenAI client
     */
    constructor(options: OpenAiExecutionToolsOptions);
    /**
     * Calls OpenAI API to use a chat model.
     */
    gptChat(prompt: Pick<Prompt, 'content' | 'modelRequirements' | 'expectFormat'>): Promise<PromptChatResult>;
    /**
     * Calls OpenAI API to use a complete model.
     */
    gptComplete(prompt: Pick<Prompt, 'content' | 'modelRequirements'>): Promise<PromptCompletionResult>;
    /**
     * Default model for chat variant.
     */
    private getDefaultChatModel;
    /**
     * Default model for completion variant.
     */
    private getDefaultCompletionModel;
    /**
     * List all available OpenAI models that can be used
     */
    listModels(): Array<AvailableModel>;
}
/**
 * TODO: [üß†][üßô‚Äç‚ôÇÔ∏è] Maybe there can be some wizzard for thoose who want to use just OpenAI
 * TODO: Maybe Create some common util for gptChat and gptComplete
 * TODO: Maybe make custom OpenaiError
 */
