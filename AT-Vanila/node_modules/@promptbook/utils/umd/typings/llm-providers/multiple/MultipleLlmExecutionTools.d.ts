import type { AvailableModel } from '../../execution/LlmExecutionTools';
import type { LlmExecutionTools } from '../../execution/LlmExecutionTools';
import type { PromptChatResult } from '../../execution/PromptResult';
import type { PromptCompletionResult } from '../../execution/PromptResult';
import type { Prompt } from '../../types/Prompt';
/**
 * Multiple LLM Execution Tools is a proxy server that uses multiple execution tools internally and exposes the executor interface externally.
 *
 * @see https://github.com/webgptorg/promptbook#multiple-server
 */
export declare class MultipleLlmExecutionTools implements LlmExecutionTools {
    /**
     * Array of execution tools in order of priority
     */
    private llmExecutionTools;
    /**
     * Gets array of execution tools in order of priority
     */
    constructor(...llmExecutionTools: Array<LlmExecutionTools>);
    /**
     * Calls the best available chat model
     */
    gptChat(prompt: Prompt): Promise<PromptChatResult>;
    /**
     * Calls the best available completion model
     */
    gptComplete(prompt: Prompt): Promise<PromptCompletionResult>;
    /**
     * Calls the best available model
     */
    private gptCommon;
    /**
     * List all available models that can be used
     * This liost is a combination of all available models from all execution tools
     */
    listModels(): Promise<Array<AvailableModel>>;
}
